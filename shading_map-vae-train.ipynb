{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mlima/opt/anaconda3/lib/python3.9/site-packages/requests/__init__.py:109: RequestsDependencyWarning: urllib3 (1.26.9) or chardet (5.0.0)/charset_normalizer (2.0.12) doesn't match a supported version!\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "''' imports '''\n",
    "\n",
    "# plotting with matplotlib\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "from solar_module import SolarModule, generate_shading, generate_gaussian\n",
    "from string_to_embedding import string_to_embedding, super_to_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            Shading Map\n",
      "0     [[ 5.38764783  4.00788436  1.37976347  0.59132...\n",
      "1     [[ 3.95244216  3.02056555  0.          0.99614...\n",
      "2     [[ 1.24570447  4.16666667  5.32646048  2.10481...\n",
      "3     [[ 4.47183099  5.88028169  2.78169014  2.07746...\n",
      "4     [[ 0.          3.26633166 10.          6.78391...\n",
      "...                                                 ...\n",
      "9995  [[ 1.70454545  4.62662338  5.56006494  3.57142...\n",
      "9996  [[ 0.59618442  0.91414944  2.38473768  3.97456...\n",
      "9997  [[ 0.15337423  0.84355828  2.07055215  2.41564...\n",
      "9998  [[ 0.          0.73333333  5.53333333  5.86666...\n",
      "9999  [[ 0.03787879  0.45454545  2.27272727  4.84848...\n",
      "\n",
      "[10000 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "''' build 'image' dataset (shading maps as 2d array) as list '''\n",
    "read_in = pd.read_csv('shading_series.csv', header=0, names=['Shading Map'], usecols=[1])\n",
    "print(read_in)\n",
    "def convert_to_array(string):\n",
    "    a = np.matrix(string).reshape(10, 6)\n",
    "    a = np.array(a)\n",
    "    a = np.pad(a, ((0,), (2,)))\n",
    "    a = np.divide(a, 10)\n",
    "    return a\n",
    "l = [convert_to_array(s) for s in read_in['Shading Map']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAARgAAAEYCAYAAACHjumMAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAAAFpklEQVR4nO3dvYteZR7H4fPMjItKEESyQVBEcdnWQhC2XjtLRRC22MXC3sL/wEpRF20EBQVlsVECwWLRxiJVGkGERRYs1tfEKGqUzMuxSBe2SCSfX3TmusrDge8phg/3wxT3Zl3XBaCwdb0/ADi8BAbICAyQERggIzBAZudqXt5sNofuX05/PnZiZOfHvZmWf7V7YWRn9+CnkZ1LZv7s1nV3ZOeQOruu6/HLH15VYC7ZvhYf85vx6n1/G9k5fe7YyM6LX5wZ2fnywocjO8uyLPsHF0d2Lu5+NbJzOO1/+v+e+okEZAQGyAgMkBEYICMwQEZggIzAABmBATICA2QEBsgIDJARGCAjMEBGYICMwAAZgQEyAgNkBAbICAyQERggIzBARmCAjMAAGYEBMpt1vfJb8y7d7Hi4Ll7b+/7kyM7WqfdHdp586paRnc9+3BvZWZZl+d/uDyM757fOjex8dP6NkZ1Z+2fWdb3/8qdOMEBGYICMwAAZgQEyAgNkBAbICAyQERggIzBARmCAjMAAGYEBMgIDZAQGyAgMkBEYICMwQEZggIzAABmBATICA2QEBsgIDJARGCAjMEBGYIDMzvX+gOttvenmkZ39hx8a2Xn2YOYq3G9PzVyzuizL8sF/7hjZ+fcX94zsfHR+ZOY3wQkGyAgMkBEYICMwQEZggIzAABmBATICA2QEBsgIDJARGCAjMEBGYICMwAAZgQEyAgNkBAbICAyQERggIzBARmCAjMAAGYEBMgIDZAQGyGzWdb3ylzebdVm2w8+Zt/fZGyM76x9PjOwcRtsn3x3Z+e6dz0d2bn39zZGdWftn1nW9//KnTjBARmCAjMAAGYEBMgIDZAQGyAgMkBEYICMwQEZggIzAABmBATICA2QEBsgIDJARGCAjMEBGYICMwAAZgQEyAgNkBAbICAyQERggIzBA5sjf7Pjxg0+M7Nz77sMjO2P29+a2tnfmtgbcsPPX6/0JATc7AsMEBsgIDJARGCAjMEBGYICMwAAZgQEyAgNkBAbICAyQERggIzBARmCAjMAAGYEBMgIDZAQGyAgMkBEYICMwQEZggIzAABmBATICA2QO15V5v8KLHx8f2Xl+ZGXO1utvj20d/P2RsS2uLScYICMwQEZggIzAABmBATICA2QEBsgIDJARGCAjMEBGYICMwAAZgQEyAgNkBAbICAyQERggIzBARmCAjMAAGYEBMgIDZAQGyAgMkBEYICMwQObIXx37z6fPj+wcjKzMcZ0rV8IJBsgIDJARGCAjMEBGYICMwAAZgQEyAgNkBAbICAyQERggIzBARmCAjMAAGYEBMgIDZAQGyAgMkBEYICMwQEZggIzAABmBATICA2QEBsgc+Zsd33rulpGdnRfeG9n55uL2yM5jf/lkZGdZluXGlx4f2+LacoIBMgIDZAQGyAgMkBEYICMwQEZggIzAABmBATICA2QEBsgIDJARGCAjMEBGYICMwAAZgQEyAgNkBAbICAyQERggIzBARmCAjMAAGYEBMkf+ZsdH/vHlyM7pf83cIPnMJz+P7Owe/GlkZ1mW5dHHXhvZuXDhDyM7R4kTDJARGCAjMEBGYICMwAAZgQEyAgNkBAbICAyQERggIzBARmCAjMAAGYEBMgIDZAQGyAgMkBEYICMwQEZggIzAABmBATICA2QEBsgIDJARGCBz5K+OfeXl20d23v/8YGTn7NZ/R3YeuO2GkZ1lWZZjd+6N7Lx66u6RnaPECQbICAyQERggIzBARmCAjMAAGYEBMgIDZAQGyAgMkBEYICMwQEZggIzAABmBATICA2QEBsgIDJARGCAjMEBGYICMwAAZgQEyAgNkBAbIbNZ1vfKXN5uvl2X5tPsc4HfqrnVdj1/+8KoCA3A1/EQCMgIDZAQGyAgMkBEYICMwQEZggIzAABmBATK/ALwvmc4Q+2g8AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "''' select cell image by label '''\n",
    "\n",
    "_cell = l[0]\n",
    "\n",
    "# diplay images\n",
    "_w = 4; _h = 4; fig = plt.figure(figsize = (_w, _h))\n",
    "ax = []; ax.append(fig.add_subplot(111))#; ax.append(fig.add_subplot(122))\n",
    "\n",
    "ax[0].imshow(_cell, cmap = 'magma')\n",
    "\n",
    "ax[0].set_xticks([]); ax[0].set_yticks([])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "''' dataset components '''\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, x_tensor, y_tensor):\n",
    "        self.x = x_tensor\n",
    "        self.y = y_tensor\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return (self.x[index], self.y[index])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' reshape, convert to tensors, init dataset '''\n",
    "\n",
    "# init random with seed\n",
    "#random.seed(252)\n",
    "\n",
    "# inplace shuffle dataset (augmentations)\n",
    "#random.shuffle(data)\n",
    "\n",
    "# stack and reshape cell image dataset (n images, 1 channel, width, height)\n",
    "#__data = np.stack(_data).reshape(len(_data),1,_data[0].shape[0],_data[0].shape[1])\n",
    "#__data.shape\n",
    "\n",
    "# list of tensors, reshaped with 1 channel\n",
    "_data = [ torch.Tensor(_.reshape(1,_.shape[0],_.shape[1])) for _ in l ]\n",
    "\n",
    "# initialise custom dataset from cell images\n",
    "dataset = CustomDataset(_data, list(range(len(_data))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10, 10]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(_data[0].shape[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10, 10])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_data[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' use cuda else cpu for compute '''\n",
    "\n",
    "# check available compute device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' define cnn vae model '''\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    #def __init__(self, imgChannels=1, featureDim=32*20*20, zDim=64):\n",
    "    def __init__(self, imgChannels=1, fDim=8, kern=3, zDim=32, lDim=1024, imgShape = (10,10)):\n",
    "        super(VAE, self).__init__()\n",
    "        \n",
    "        self.fDim = fDim\n",
    "        self.imgShape = imgShape\n",
    "        self.featureDim = 2*self.fDim*(self.imgShape[0])*(self.imgShape[1])#*4//kern\n",
    "        \n",
    "        #self.featureDim = _in - (kern-1)-1\n",
    "        \n",
    "        #self.featureDim = 2*self.fDim*20*20\n",
    "        #print(self.featureDim)\n",
    "        \n",
    "        self.pad = (kern-1)//2\n",
    "\n",
    "        # Initializing the 2 convolutional layers and 2 full-connected layers for the encoder\n",
    "        self.encConv1 = nn.Conv2d(imgChannels, fDim, kern, padding = self.pad ) # (in_channels, out_channels, kernel)\n",
    "        #self.encConv2 = nn.Conv2d(fDim, 2*fDim, kern, padding = 'same')\n",
    "        self.encConv2 = nn.Conv2d(fDim, 2*fDim, kern, padding = self.pad )\n",
    "        self.encFC1 = nn.Linear(lDim, zDim)\n",
    "        self.encFC2 = nn.Linear(lDim, zDim)\n",
    "        self.encFC3 = nn.Linear(self.featureDim, lDim) # large linear layer between layers 1 and 2\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "        # 6 conv layers, 6 -> 16 -> 32 -> 64 -> 32 -> 16 ->(linear) -> ... -> 10,800\n",
    "        \n",
    "        # embedding dimensions are (10, 6, 10, 6, 3) 10,800 nodes or (60, 60, 3)\n",
    "\n",
    "        # Initializing the fully-connected layer and 2 convolutional layers for decoder\n",
    "        self.decFC2 = nn.Linear(zDim, lDim)\n",
    "        self.decFC1 = nn.Linear(lDim, self.featureDim)\n",
    "        self.decConv1 = nn.ConvTranspose2d(2*fDim, fDim, kern, padding = self.pad )\n",
    "        self.decConv2 = nn.ConvTranspose2d(fDim, imgChannels, kern, padding = self.pad )\n",
    "\n",
    "    def encoder(self, x):\n",
    "\n",
    "        # Input is fed into 2 convolutional layers sequentially\n",
    "        # The output feature map are fed into 2 fully-connected layers to predict mean (mu) and variance (logVar)\n",
    "        # Mu and logVar are used for generating middle representation z and KL divergence loss\n",
    "        #print('enc ', x.shape)\n",
    "        x = F.relu(self.encConv1(x))\n",
    "        #print('conv1 ', x.shape)\n",
    "        x = F.relu(self.encConv2(x))\n",
    "        #print('conv2 ', x.shape)\n",
    "        x = x.view(-1, self.featureDim)\n",
    "        x = self.encFC3(x)\n",
    "        x = self.relu(x)\n",
    "        mu = self.encFC1(x)\n",
    "        logVar = self.encFC2(x)\n",
    "        return mu, logVar\n",
    "\n",
    "    def reparameterize(self, mu, logVar):\n",
    "\n",
    "        #Reparameterization takes in the input mu and logVar and sample the mu + std * eps\n",
    "        std = torch.exp(logVar/2)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + std * eps\n",
    "\n",
    "    def decoder(self, z):\n",
    "\n",
    "        # z is fed back into a fully-connected layers and then into two transpose convolutional layers\n",
    "        # The generated output is the same size of the original input.\n",
    "        x = F.relu(self.decFC2(z))\n",
    "        x = F.relu(self.decFC1(x))\n",
    "        #print('dec ', x.shape)\n",
    "        x = x.view(-1, 2*self.fDim, self.imgShape[0], self.imgShape[1])\n",
    "        #x = x.view(-1, 2*self.fDim, 10, 10)\n",
    "        #print('decv ', x.shape)\n",
    "        x = F.relu(self.decConv1(x))\n",
    "        x = torch.sigmoid(self.decConv2(x))\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # The entire pipeline of the VAE: encoder -> reparameterization -> decoder\n",
    "        # output, mu, and logVar are returned for loss computation\n",
    "        mu, logVar = self.encoder(x)\n",
    "        z = self.reparameterize(mu, logVar)\n",
    "        out = self.decoder(z)\n",
    "        return out, mu, logVar\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Initialize Hyperparameters \"\"\"\n",
    "\n",
    "# init hyper params\n",
    "batch_size = 32 # 32-64 is advisable\n",
    "learning_rate = 1e-3 # sensitivity of gradient descent\n",
    "num_epochs = 1 # iterations over entire dataset \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Create dataloaders to feed data into the neural network \"\"\"\n",
    "\n",
    "## note: better to split dataset into ~80:20 train:test, below simply entire dataset\n",
    "\n",
    "# training dataset\n",
    "train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# test/validation dataset\n",
    "test_loader = torch.utils.data.DataLoader(dataset, batch_size=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10, 10])\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAANAAAADQCAYAAAB2pO90AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAAAEYUlEQVR4nO3dv2vcdRzH8ff10iQ1TWlMS1GkiSAoWDA4+AOKIA5uIuLgqIt/hJN/hIuC6CB0cXHp1OIgODg4iIM6iEFiwDZIjKmoab7Olg6mr3eT3PF4jEd48T24532PhE9uNAxDAffmxFFfAEwyAUFAQBAQEAQEBIGZg/zw7GhumK+F+3Ut92z/wb5r2jvVNlXV+AvOYdy31W10u29r9pfdvrFGO/XbzWEYzt/5+IECmq+Fenb0Ut9VNdl5+bm2rZtro7atE3+3TdU/i71/bhhm+vbmtvrqvvjul21bna4Nn67f7XEf4SAgIAgICAICgoCAICAgCAgIAgKCgIAgICAICAgCAoKAgCAgIAgICAICgoCAIHCgE6nH1X7js1h8cqtt69rax21bH21fatuqqvrw++fbtva2z7RtTRp3IAgICAICgoCAICAgCAgIAgKCgIAgICAICAgCAoKAgCAgIAgICAICgoCAICAgCAgIAlNxpHv5i422rc3FR9q2Plh9qm3rwsnttq2qqj83TrdtPfp547cpTxh3IAgICAICgoCAICAgCAgIAgKCgIAgICAICAgCAoKAgCAgIAgICAICgoCAICAgCAgIAlNxpHtv/ee2reVvz7VtPT6/2bb16sIfbVtVVW++/n7b1gvX327bmrQXpDsQBAQEAQFBQEAQEBAEBAQBAUFAQBAQEAQEBAEBQUBAEBAQBAQEAQFBQEAQEBAEBASBSTtBe1ejmb6nsbMy17bVfQz7uNp4se99+LHP2qYOhTsQBAQEAQFBQEAQEBAEBAQBAUFAQBAQEAQEBAEBQUBAEBAQBAQEAQFBQEAQEBAEBASBqTjSXeNx29Tuw95TDmp8a3TUl3BkvFogICAICAgCAoKAgCAgIAgICAICgoCAICAgCAgIAgKCgIAgICAICAgCAoKAgCAgIAgICALT8T8R9oe2qcX1/baty9+81ra1trzRtlVV9crS121bS9+1TU0cdyAICAgCAoKAgCAgIAgICAICgoCAICAgCAgIAgKCgIAgICAICAgCAoKAgCAgIAgICAJTcaR77/Kltq0zP+y0bf301YW2rasXl9q2qqqu7q61bc2t9n3N/dm2pcPhDgQBAUFAQBAQEAQEBAEBQUBAEBAQBAQEAQFBQEAQEBAEBAQBAUFAQBAQEAQEBAEBQWAqjnT/+Fbf1uonp9q2Hthsm6q/zo37xqqqZvq+jfzk07+3bU0adyAICAgCAoKAgCAgIAgICAICgoCAICAgCAgIAgKCgIAgICAICAgCAoKAgCAgIAgICAJTcaR75aGttq3ZG6fbtpZuD21bM7dm27aqqsZv/Nq29d4TV9q23qln2rYOgzsQBAQEAQFBQEAQEBAEBAQBAUFAQBAQEAQEBAEBQUBAEBAQBAQEAQFBQEAQEBAEBASB0TD8/2PHo9HoRlWt37/LgWNrZRiG83c+eKCAgP/yEQ4CAoKAgCAgIAgICAICgoCAICAgCAgIAv8CNgNrm8p38yUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 216x216 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2618)\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Check image dataset \"\"\"\n",
    "    \n",
    "# get single random image batch from test dataset\n",
    "for d in random.sample(list(test_loader), 1):\n",
    "\n",
    "    # unpack image data, label from sample\n",
    "    imgs, _ = d\n",
    "\n",
    "    # push images to gpu\n",
    "    imgs = imgs.to(device)\n",
    "\n",
    "    print(imgs[0].shape)\n",
    "\n",
    "    # get single image from gpu\n",
    "    img = np.transpose(imgs[0].cpu().numpy(), [1,2,0])\n",
    "\n",
    "    plt.figure(figsize = (3,3))\n",
    "\n",
    "    # plot input image\n",
    "    plt.subplot(111)\n",
    "    plt.imshow(np.squeeze(img))\n",
    "    plt.xticks([]); plt.yticks([])\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(_[0])\n",
    "\n",
    "    break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Initialize the network \"\"\"\n",
    "\n",
    "# init vae model\n",
    "#net = VAE().to(device)\n",
    "\n",
    "# init model with specified hyperparams\n",
    "#net = VAE(fDim=16, kern=(1), zDim=32, imgShape=(25,25)).to(device)\n",
    "net = VAE().to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Initialize the Adam optimizer \"\"\"\n",
    "\n",
    "# init adam optimiser\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.3075, -0.2083, -0.0278,  0.0251, -0.1115,  0.0657,  0.3169, -0.2905])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.encConv1.bias.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x216 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1449)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "The following part takes a random image from test loader to feed into the VAE.\n",
    "Both the original image and generated image from the distribution are shown.\n",
    "\"\"\"\n",
    "\n",
    "# set model eval state\n",
    "net.eval()\n",
    "\n",
    "# without compute gradients\n",
    "with torch.no_grad():\n",
    "    \n",
    "    # get single random image batch from test dataset\n",
    "    for d in random.sample(list(test_loader), 1):\n",
    "        \n",
    "        # unpack image data, label from sample\n",
    "        imgs, _ = d\n",
    "        \n",
    "        # push images to gpu\n",
    "        imgs = imgs.to(device)\n",
    "        \n",
    "        # get single image from gpu\n",
    "        img = np.transpose(imgs[0].cpu().numpy(), [1,2,0])\n",
    "        \n",
    "        plt.figure(figsize = (6,3))\n",
    "        \n",
    "        # plot input image\n",
    "        plt.subplot(121)\n",
    "        plt.imshow(np.squeeze(img))\n",
    "        plt.xticks([]); plt.yticks([])\n",
    "        \n",
    "        # compute output image\n",
    "        out, mu, logVAR = net(imgs)\n",
    "        \n",
    "        # get output image from gpu, reshape\n",
    "        outimg = np.transpose(out[0].cpu().numpy(), [1,2,0])\n",
    "        \n",
    "        # plot output image\n",
    "        plt.subplot(122)\n",
    "        plt.imshow(np.squeeze(outimg))\n",
    "        plt.xticks([]); plt.yticks([])\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(_[0])\n",
    "        \n",
    "        break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0.0000, 0.0000, 0.2081, 0.3901, 0.2503, 0.2243, 0.1138, 0.0033,\n",
      "           0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.2861, 0.1235, 0.1951, 0.2633, 0.1495, 0.5137,\n",
      "           0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.1430, 0.4064, 0.8908, 0.6827, 0.3056,\n",
      "           0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.1170, 0.4746, 1.0000, 0.8973, 0.1918, 0.3251,\n",
      "           0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.6437, 1.0000, 1.0000, 0.5982, 1.0000, 1.0000,\n",
      "           0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.7705, 1.0000, 0.9493, 1.0000, 0.8420, 0.6372,\n",
      "           0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 1.0000, 0.5949, 1.0000, 0.8713, 0.2633, 0.3934,\n",
      "           0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.6372, 1.0000, 1.0000, 0.4291, 0.5299, 0.3153,\n",
      "           0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.4844, 0.1853, 0.3739, 0.8290, 0.7835, 0.4909,\n",
      "           0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.2308, 0.4454, 0.7607, 1.0000, 0.6795, 0.5592,\n",
      "           0.0000, 0.0000]]]])\n"
     ]
    }
   ],
   "source": [
    "print(imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "scrolled": false,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Loss 0.3454967439174652\n",
      "Epoch 1: Loss 0.3508634567260742\n",
      "Epoch 2: Loss 0.37597084045410156\n",
      "Epoch 3: Loss 0.35750797390937805\n",
      "Epoch 4: Loss 0.36550983786582947\n",
      "Epoch 5: Loss 0.35332709550857544\n",
      "Epoch 6: Loss 0.34153908491134644\n",
      "Epoch 7: Loss 0.3319317102432251\n",
      "Epoch 8: Loss 0.349409282207489\n",
      "Epoch 9: Loss 0.3609241843223572\n",
      "Epoch 10: Loss 0.366657018661499\n",
      "Epoch 11: Loss 0.3780399262905121\n",
      "Epoch 12: Loss 0.35723185539245605\n",
      "Epoch 13: Loss 0.34744930267333984\n",
      "Epoch 14: Loss 0.3502640426158905\n",
      "Epoch 15: Loss 0.3660466969013214\n",
      "Epoch 16: Loss 0.354277640581131\n",
      "Epoch 17: Loss 0.3504391312599182\n",
      "Epoch 18: Loss 0.3610565960407257\n",
      "Epoch 19: Loss 0.3668939769268036\n",
      "Epoch 20: Loss 0.33947980403900146\n",
      "Epoch 21: Loss 0.34979888796806335\n",
      "Epoch 22: Loss 0.3578239679336548\n",
      "Epoch 23: Loss 0.34364479780197144\n",
      "Epoch 24: Loss 0.36902862787246704\n",
      "Epoch 25: Loss 0.34927037358283997\n",
      "Epoch 26: Loss 0.34972140192985535\n",
      "Epoch 27: Loss 0.3599760830402374\n",
      "Epoch 28: Loss 0.3489381968975067\n",
      "Epoch 29: Loss 0.36617740988731384\n",
      "Epoch 30: Loss 0.3787493407726288\n",
      "Epoch 31: Loss 0.3564591705799103\n",
      "Epoch 32: Loss 0.35943543910980225\n",
      "Epoch 33: Loss 0.36172205209732056\n",
      "Epoch 34: Loss 0.3553997874259949\n",
      "Epoch 35: Loss 0.3234587013721466\n",
      "Epoch 36: Loss 0.3764459192752838\n",
      "Epoch 37: Loss 0.3637319803237915\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [92]\u001b[0m, in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;66;03m#loss = (optimised power value from labels - observed power)^2 for supervised model\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \n\u001b[1;32m     35\u001b[0m     \u001b[38;5;66;03m# Backpropagation based on the loss\u001b[39;00m\n\u001b[1;32m     36\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 37\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m: Loss \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(epoch, loss))\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/_tensor.py:363\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    355\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    356\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    357\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    361\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[1;32m    362\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[0;32m--> 363\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Training the network for a given number of epochs\n",
    "The loss after every epoch is printed\n",
    "\"\"\"\n",
    "\n",
    "# set train state\n",
    "net.train()\n",
    "\n",
    "num_epochs = 100\n",
    "\n",
    "# iterate over epochs\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    # iterate over training dataset by batch\n",
    "    for idx, d in enumerate(train_loader, 0):\n",
    "        \n",
    "        # unpack batch images and labels\n",
    "        imgs, _ = d\n",
    "        \n",
    "        # push images to gpu\n",
    "        imgs = imgs.to(device)\n",
    "        \n",
    "        #print('batch ',imgs.shape)\n",
    "\n",
    "        # Feeding a batch of images into the network to obtain the output image, mu, and logVar\n",
    "        out, mu, logVar = net(imgs)\n",
    "        \n",
    "        #print('out ', out.shape)\n",
    "\n",
    "        # The loss is the BCE loss combined with the KL divergence to ensure the distribution is learnt\n",
    "        kl_divergence = 0.5 * torch.sum(-1 - logVar + mu.pow(2) + logVar.exp())\n",
    "        loss = F.binary_cross_entropy(out, imgs) + kl_divergence #TODO: de-weight the kl_divergence\n",
    "        #loss = (optimised power value from labels - observed power)^2 for supervised model\n",
    "\n",
    "        # Backpropagation based on the loss\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print('Epoch {}: Loss {}'.format(epoch, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaYAAADUCAYAAADX7Id9AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAAAHF0lEQVR4nO3dy4uddx3H8e9zzkxmmjE326G1WEq1VmvwAgXbhXThhYpuCoILdSEIXgp2Ieg/IUJXxZULxYWoIEIXQkTajbQbFbWtCE1qa0laS2pmkszkZM7joiAisbn0Oz2fSV+v5TDnw5PJPOc9z8DwG8ZxLABIMVn0BQDAfxMmAKIIEwBRhAmAKMIEQBRhAiDK0tV88r5hZVyttd26ljdlmPQ1dv2D59q2qqoOT+ZtWxfGnbatqqqt8aq+Bd7QyRPvbNuqqqrN8717TbbqbF0Yt4drfX3yfTQ/0ndds+5/4nLffTSZ5P6ZzHx+zd9al7RyPPM+qqraqNP/HMdx/X8/flXvSqu1VvcOn+y7qkaT/X13wdd/+ce2raqqB9c227ZevNi3VVX19IUjbVvf++qX27aqqiaP/751r8uT42/e1OuT76ONB+5r23r5Y21Tr7tlu23qhv19W1W9Mbmwvdy2VVX1ni/+oXWv07Hx589f6uN+lQdAFGECIIowARBFmACIIkwARBEmAKIIEwBRhAmAKMIEQBRhAiCKMAEQRZgAiCJMAEQRJgCiCBMAUYQJgCh9x5cu2N8f/kjb1nd/9uG2raqq1S/8sHFtpXGr6jONB6Y9cnKjbauqqvesXq7EqXv7to7ec6JvrKo+fdMzbVu37Xu1bavb37Zvbt07Vgda994KnpgAiCJMAEQRJgCiCBMAUYQJgCjCBEAUYQIgijABEEWYAIgiTABEESYAoggTAFGECYAowgRAFGECIIowARBFmACIIkwARBEmAKIsLfoCuhw8MW/b+t33f9C2VVX1i82DbVuff8eZtq2qqg898lDb1s23brVtVVVN/9o6xxWYH7rYtvWJm55t26qq+tTaM21bhyd97xdVVefGvq2dsft54UDz3u7zxARAFGECIIowARBFmACIIkwARBEmAKIIEwBRhAmAKMIEQBRhAiCKMAEQRZgAiCJMAEQRJgCiCBMAUYQJgCjCBEAUYQIgynVztPprd+U2djr0HuPc6fbPHm/bevrou9q2qqre99vWOa7AsNz3vXrr8um2raqq9Wnf+eWHJje0bVVVbc6327bWJn1be1XuuzkAb0vCBEAUYQIgijABEEWYAIgiTABEESYAoggTAFGECYAowgRAFGECIIowARBFmACIIkwARBEmAKIIEwBRhAmAKMIEQBRhAiDK0qIvoMt4dGPRl/B/Pbi22bb12LnVtq2qquO/vqNt67lvPdq2VVX1QH20dY/Lmy7N27ZWh1nbVlXVtIa2rUnjVlXVTo2te293npgAiCJMAEQRJgCiCBMAUYQJgCjCBEAUYQIgijABEEWYAIgiTABEESYAoggTAFGECYAowgRAFGECIIowARBFmACIIkwARLlujlZ/9uM/XvQlvCU+t3+rd6/xOPSfbNzYtsViTKZ9R6tPh76tqqrJ0HsceqpJ9X7d9iJPTABEESYAoggTAFGECYAowgRAFGECIIowARBFmACIIkwARBEmAKIIEwBRhAmAKMIEQBRhAiCKMAEQRZgAiCJMAEQRJgCiCBMAUZYWfQGJ7n/oa617p+/s+zL/6duPtm11+9KBV1v3flS3te5xedPpvG1ra76vbauqalpD39bQ+zN577X1/R/sVZ6YAIgiTABEESYAoggTAFGECYAowgRAFGECIIowARBFmACIIkwARBEmAKIIEwBRhAmAKMIEQBRhAiCKMAEQRZgAiCJMAES5bo5Wv+8732jbOvzwC21bVVVPfOCx1r1Ud/yq90j6u+qp1j0ubz7PPSJ8Nvbt7TRuVVWdbdx75eLBtq29yhMTAFGECYAowgRAFGECIIowARBFmACIIkwARBEmAKIIEwBRhAmAKMIEQBRhAiCKMAEQRZgAiCJMAEQRJgCiCBMAUYQJgCjCBECUpUVfQJe1k7O2redevrFtq6rqnp9+s21r/akzbVtVVf+6+0Db1p3/2G7bYjHm86Ft68zOattWVdXJnb6ts+O5vrGqOrWzr23rpdmRtq29yhMTAFGECYAowgRAFGECIIowARBFmACIIkwARBEmAKIIEwBRhAmAKMIEQBRhAiCKMAEQRZgAiCJMAEQRJgCiCBMAUYQJgCjXzdHqK39+oW3rvV/pPb58vrXVtjVZX2/bqqoa3t93tPrOyrRtq8pPTYswO7/ctvWX8+9u26qqmo19b1erk1nbVlXVS7PDbVvHTt3dtvW6F5v3dp97H4AowgRAFGECIIowARBFmACIIkwARBEmAKIIEwBRhAmAKMIEQBRhAiCKMAEQRZgAiCJMAEQRJgCiCBMAUYQJgCjCBEAUYQIgyjCO45V/8jC8UlXP797lwJ5w+ziO69f6YvcR/Mcl76WrChMA7Da/ygMgijABEEWYAIgiTABEESYAoggTAFGECYAowgRAFGECIMq/Ab4r3nuMDtHbAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x216 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1469)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "The following part takes a random image from test loader to feed into the VAE.\n",
    "Both the original image and generated image from the distribution are shown.\n",
    "\"\"\"\n",
    "\n",
    "# set model eval state\n",
    "net.eval()\n",
    "\n",
    "# without compute gradients\n",
    "with torch.no_grad():\n",
    "    \n",
    "    # get single random image batch from test dataset\n",
    "    for d in random.sample(list(test_loader), 1):\n",
    "        \n",
    "        # unpack image data, label from sample\n",
    "        imgs, _ = d\n",
    "        \n",
    "        # push images to gpu\n",
    "        imgs = imgs.to(device)\n",
    "        \n",
    "        # get single image from gpu\n",
    "        img = np.transpose(imgs[0].cpu().numpy(), [1,2,0])\n",
    "        \n",
    "        plt.figure(figsize = (6,3))\n",
    "        \n",
    "        # plot input image\n",
    "        plt.subplot(121)\n",
    "        plt.imshow(np.squeeze(img))\n",
    "        plt.xticks([]); plt.yticks([])\n",
    "        \n",
    "        # compute output image\n",
    "        out, mu, logVAR = net(imgs)\n",
    "        \n",
    "        # get output image from gpu, reshape\n",
    "        outimg = np.transpose(out[0].cpu().numpy(), [1,2,0])\n",
    "        \n",
    "        # plot output image\n",
    "        plt.subplot(122)\n",
    "        plt.imshow(np.squeeze(outimg))\n",
    "        plt.xticks([]); plt.yticks([])\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(_[0])\n",
    "        \n",
    "        break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "''' save model checkpoint '''\n",
    "\n",
    "# set model checkpoint path inc. hyperparams\n",
    "_path = 'f{}-k{}-z{}-i{}-v{}.pth'.format(8,3,256,115,'01')\n",
    "\n",
    "torch.save({\n",
    "            #'epoch': epoch,\n",
    "            #'model_state_dict': model.state_dict(),\n",
    "            'model_state_dict': net.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            #'loss': loss,\n",
    "}, _path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' load model checkpoint from file '''\n",
    "\n",
    "# init model and optimiser\n",
    "#model = TheModelClass(*args, **kwargs)\n",
    "#optimizer = TheOptimizerClass(*args, **kwargs)\n",
    "\n",
    "#_path = '../data/model/cnn-vae-f16-k5-z128-i121-v01.pth'\n",
    "#_path = '../data/model/cnn-vae-f16-k3-z128-i115-v01.pth'\n",
    "_path = '../data/model/cnn-vae-f8-k3-z256-i115-v01.pth'\n",
    "\n",
    "# load checkpoint\n",
    "checkpoint = torch.load(_path)\n",
    "\n",
    "# update model\n",
    "#model.load_state_dict(checkpoint['model_state_dict'])\n",
    "net.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "# update optimiser\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "# set epoch and loss\n",
    "#epoch = checkpoint['epoch']\n",
    "#loss = checkpoint['loss']\n",
    "\n",
    "# set model train/eval state\n",
    "#model.eval()\n",
    "#model.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
