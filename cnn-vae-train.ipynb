{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.08479421 1.         0.38648363 1.         0.63341\n",
      "  1.         1.         0.72480497 0.41704269 1.         1.\n",
      "  1.         0.47035792 1.         1.        ]\n",
      " [1.         0.60346863 0.69179263 0.85943205 1.         0.80945606\n",
      "  1.         1.         1.         1.         1.         1.\n",
      "  1.         1.         0.48942727 1.        ]\n",
      " [0.67555665 1.         1.         1.         0.43102796 1.\n",
      "  0.76705267 1.         0.80524996 1.         1.         1.\n",
      "  1.         1.         0.40525798 1.        ]\n",
      " [0.88320748 1.         0.36400685 1.         0.62902701 1.\n",
      "  1.         1.         1.         0.06760475 0.67392958 1.\n",
      "  0.63672962 0.13017189 0.38987651 1.        ]\n",
      " [1.         1.         1.         1.         1.         0.43014697\n",
      "  0.57939927 0.44065353 0.90665765 0.67006059 0.12263469 1.\n",
      "  0.96663279 1.         1.         1.        ]\n",
      " [0.81047191 1.         0.73273044 1.         1.         1.\n",
      "  1.         1.         1.         1.         0.11919695 1.\n",
      "  1.         0.77090821 1.         1.        ]\n",
      " [1.         1.         0.35561133 1.         0.45082656 0.43304102\n",
      "  1.         1.         1.         1.         1.         0.04751078\n",
      "  0.23889776 1.         1.         0.75571996]\n",
      " [1.         1.         1.         1.         1.         1.\n",
      "  1.         1.         1.         0.90833847 1.         1.\n",
      "  1.         0.3208769  0.5962014  0.22578584]\n",
      " [1.         0.01325579 1.         1.         1.         1.\n",
      "  0.17063255 1.         0.65083978 1.         1.         0.53407071\n",
      "  1.         1.         1.         1.        ]\n",
      " [1.         1.         1.         1.         1.         0.46636141\n",
      "  1.         0.35947726 1.         0.79168342 1.         0.33275475\n",
      "  0.82865333 1.         0.13990967 0.06831426]\n",
      " [1.         1.         1.         1.         1.         1.\n",
      "  1.         1.         1.         1.         0.13457186 1.\n",
      "  0.0822247  1.         0.3188379  1.        ]\n",
      " [1.         1.         0.7813955  1.         1.         1.\n",
      "  1.         1.         0.05636833 1.         1.         1.\n",
      "  0.3672554  0.9244067  1.         1.        ]\n",
      " [1.         0.1344889  0.57874602 1.         1.         1.\n",
      "  1.         1.         0.92133016 0.08889628 1.         1.\n",
      "  1.         1.         1.         1.        ]\n",
      " [0.11472106 1.         1.         0.42744274 1.         0.16866533\n",
      "  1.         0.61628363 0.908524   0.13764158 0.37031059 0.01662774\n",
      "  0.42307207 1.         1.         0.35367914]\n",
      " [0.31339403 1.         0.54299797 0.49154108 1.         1.\n",
      "  1.         1.         0.07153049 0.80756238 1.         1.\n",
      "  1.         1.         1.         0.42360975]\n",
      " [1.         1.         1.         1.         1.         1.\n",
      "  1.         1.         1.         1.         1.         1.\n",
      "  1.         1.         1.         1.        ]]\n",
      "2022-08-24 13:51:58,287 - numexpr.utils - utils.INFO - NumExpr defaulting to 8 threads.\n"
     ]
    }
   ],
   "source": [
    "''' imports '''\n",
    "\n",
    "# plotting with matplotlib\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "from solar_module import SolarModule, generate_shading\n",
    "from string_to_embedding import string_to_embedding\n",
    "\n",
    "import pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' build 'image' dataset (shading maps as 2d array) as list '''\n",
    "l = []\n",
    "for x in range(10000):\n",
    "    array = generate_shading(3, 1, 12, 12)\n",
    "    array = np.pad(array, 2)\n",
    "    l.append(array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "''' select cell image by label '''\n",
    "\n",
    "_cell = l[0]\n",
    "\n",
    "# diplay images\n",
    "_w = 4; _h = 4; fig = plt.figure(figsize = (_w, _h))\n",
    "ax = []; ax.append(fig.add_subplot(111))#; ax.append(fig.add_subplot(122))\n",
    "\n",
    "ax[0].imshow(_cell, cmap = 'magma')\n",
    "\n",
    "ax[0].set_xticks([]); ax[0].set_yticks([])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "''' dataset components '''\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, x_tensor, y_tensor):\n",
    "        self.x = x_tensor\n",
    "        self.y = y_tensor\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return (self.x[index], self.y[index])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' reshape, convert to tensors, init dataset '''\n",
    "\n",
    "# init random with seed\n",
    "#random.seed(252)\n",
    "\n",
    "# inplace shuffle dataset (augmentations)\n",
    "#random.shuffle(data)\n",
    "\n",
    "# stack and reshape cell image dataset (n images, 1 channel, width, height)\n",
    "#__data = np.stack(_data).reshape(len(_data),1,_data[0].shape[0],_data[0].shape[1])\n",
    "#__data.shape\n",
    "\n",
    "# list of tensors, reshaped with 1 channel\n",
    "_data = [ torch.Tensor(_.reshape(1,_.shape[0],_.shape[1])) for _ in l ]\n",
    "\n",
    "# initialise custom dataset from cell images\n",
    "dataset = CustomDataset(_data, list(range(len(_data))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[16, 16]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(_data[0].shape[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 16, 16])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_data[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' use cuda else cpu for compute '''\n",
    "\n",
    "# check available compute device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' define cnn vae model '''\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    #def __init__(self, imgChannels=1, featureDim=32*20*20, zDim=64):\n",
    "    def __init__(self, imgChannels=1, fDim=8, kern=3, zDim=32, imgShape = (16,16)):\n",
    "        super(VAE, self).__init__()\n",
    "        \n",
    "        self.fDim = fDim\n",
    "        self.imgShape = imgShape\n",
    "        self.featureDim = 2*self.fDim*(self.imgShape[0])*(self.imgShape[1])#*4//kern\n",
    "        \n",
    "        #self.featureDim = _in - (kern-1)-1\n",
    "        \n",
    "        #self.featureDim = 2*self.fDim*20*20\n",
    "        #print(self.featureDim)\n",
    "        \n",
    "        self.pad = (kern-1)//2\n",
    "\n",
    "        # Initializing the 2 convolutional layers and 2 full-connected layers for the encoder\n",
    "        self.encConv1 = nn.Conv2d(imgChannels, fDim, kern, padding = self.pad ) # (in_channels, out_channels, kernel)\n",
    "        #self.encConv2 = nn.Conv2d(fDim, 2*fDim, kern, padding = 'same')\n",
    "        self.encConv2 = nn.Conv2d(fDim, 2*fDim, kern, padding = self.pad )\n",
    "        self.encFC1 = nn.Linear(self.featureDim, zDim)\n",
    "        self.encFC2 = nn.Linear(self.featureDim, zDim)\n",
    "        \n",
    "        # 6 conv layers, 6 -> 16 -> 32 -> 64 -> 32 -> 16 ->(linear) -> ... -> 10,800\n",
    "        \n",
    "        # embedding dimensions are (10, 6, 10, 6, 3) 10,800 nodes or (60, 60, 3)\n",
    "\n",
    "        # Initializing the fully-connected layer and 2 convolutional layers for decoder\n",
    "        self.decFC1 = nn.Linear(zDim, self.featureDim)\n",
    "        self.decConv1 = nn.ConvTranspose2d(2*fDim, fDim, kern, padding = self.pad )\n",
    "        self.decConv2 = nn.ConvTranspose2d(fDim, imgChannels, kern, padding = self.pad )\n",
    "\n",
    "    def encoder(self, x):\n",
    "\n",
    "        # Input is fed into 2 convolutional layers sequentially\n",
    "        # The output feature map are fed into 2 fully-connected layers to predict mean (mu) and variance (logVar)\n",
    "        # Mu and logVar are used for generating middle representation z and KL divergence loss\n",
    "        #print('enc ', x.shape)\n",
    "        x = F.relu(self.encConv1(x))\n",
    "        #print('conv1 ', x.shape)\n",
    "        x = F.relu(self.encConv2(x))\n",
    "        #print('conv2 ', x.shape)\n",
    "        x = x.view(-1, self.featureDim)\n",
    "        mu = self.encFC1(x)\n",
    "        logVar = self.encFC2(x)\n",
    "        return mu, logVar\n",
    "\n",
    "    def reparameterize(self, mu, logVar):\n",
    "\n",
    "        #Reparameterization takes in the input mu and logVar and sample the mu + std * eps\n",
    "        std = torch.exp(logVar/2)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + std * eps\n",
    "\n",
    "    def decoder(self, z):\n",
    "\n",
    "        # z is fed back into a fully-connected layers and then into two transpose convolutional layers\n",
    "        # The generated output is the same size of the original input\n",
    "        x = F.relu(self.decFC1(z))\n",
    "        #print('dec ', x.shape)\n",
    "        x = x.view(-1, 2*self.fDim, self.imgShape[0], self.imgShape[1])\n",
    "        #x = x.view(-1, 2*self.fDim, 20, 20)\n",
    "        #print('decv ', x.shape)\n",
    "        x = F.relu(self.decConv1(x))\n",
    "        x = torch.sigmoid(self.decConv2(x))\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # The entire pipeline of the VAE: encoder -> reparameterization -> decoder\n",
    "        # output, mu, and logVar are returned for loss computation\n",
    "        mu, logVar = self.encoder(x)\n",
    "        z = self.reparameterize(mu, logVar)\n",
    "        out = self.decoder(z)\n",
    "        return out, mu, logVar\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Initialize Hyperparameters \"\"\"\n",
    "\n",
    "# init hyper params\n",
    "batch_size = 32 # 32-64 is advisable\n",
    "learning_rate = 1e-3 # sensitivity of gradient descent\n",
    "num_epochs = 1 # iterations over entire dataset \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Create dataloaders to feed data into the neural network \"\"\"\n",
    "\n",
    "## note: better to split dataset into ~80:20 train:test, below simply entire dataset\n",
    "\n",
    "# training dataset\n",
    "train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# test/validation dataset\n",
    "test_loader = torch.utils.data.DataLoader(dataset, batch_size=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 16, 16])\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAANAAAADQCAYAAAB2pO90AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAAAE80lEQVR4nO3dz4uVZRzG4feIlGFYCSJpZVS2qBAN00VRiTGam0KzokXaLo2idX9ALQ0Cs1qou8rJpZaMZJAS+SuEWphZUhihJFmLguBpES2ywvl6++KZvK7lcI88CB+eWTycM2itdcCFmXSpDwATmYAgICAICAgCAoLA5Mr4isGVbUo3ta+zwND6uTtzurU249yflwKa0k3tFg+WXrxTwQQx1kZP/NvP/QkHAQFBQEAQEBAEBAQBAUFAQBAQEAQEBAEBQUBAEBAQBAQEAQFBQEAQEBAEBAQBAUFAQBAQEAQEBAEBQaD0sVZ9++DkZ6X997//UtpfP/nq0v72j9aU9kcf2FraX05GVq8t7Xdt21LaL5s1v7S/WNxAEBAQBAQEAQFBQEAQEBAEBAQBAUFAQBAQEAQEBIGhegtXVX3btvymhaX93JmnSvtuf23ep7l71pb2rQ1K+2NLNpf2j7y1u7SfKNxAEBAQBAQEAQFBQEAQEBAEBAQBAUFAQBAQEAQEBIEJ/Rau6uyq2lu4fRs29XSSujteX1/af7luY2m/YsljpX23pDZ/7tpva78wQbiBICAgCAgIAgKCgIAgICAICAgCAoKAgCAgIAgICAKD1tq4x9MG09viwdLeDlP9jlT4S9/fkTrWRg+21v7xmNINBAEBQUBAEBAQBAQEAQFBQEAQEBAEBAQBAUFAQBC4rD4Xrm8rHnq8tN8x9m5PJ6m7+8ATpf2Z764p7b9+9M3SfqJwA0FAQBAQEAQEBAEBQUBAEBAQBAQEAQFBQEAQEBAELqu3cCOr1pT2u97bWtoP09u2qkML36n9Qu3rZsvK39naHevlHOfjBoKAgCAgIAgICAICgoCAICAgCAgIAgKCgIAgMFRPeW7bs7a0nzQY/9dTdl3XHS0+zeHS2fHhaGnf91c8/hc3EAQEBAEBQUBAEBAQBAQEAQFBQEAQEBAEBAQBAUFgqN7CHXtwy6U+QuTpE/eX9j/+NnXc2zO/XlX6t/fO217aVy06vLq0/3TBttL+ztfWl/Y3dPtK+4vFDQQBAUFAQBAQEAQEBAEBQUBAEBAQBAQEAQFBQEAQGKq3cMvnLCrtb9lX63/j7E9K+6oDO+4q7b9Yt7Gnk9S99MO80v6pm/f3dJI/ff587f9m2Svz+znIebiBICAgCAgIAgKCgIAgICAICAgCAoKAgCAgIAgICAJD9Rbu6IYFpf37s9/o6SQXps+3bbfufqa0/2rp5tL+5ZlHSvsXTt5T2nfXfVPbTxBuIAgICAICgoCAICAgCAgIAgKCgIAgICAICAgCAoLAUL2FO75yuN62rVgwUtq3n86W9juPj/9z6qpv2+598dnSfu+rm0r7Q6dvLO27Wf1+jtyl4gaCgIAgICAICAgCAoKAgCAgIAgICAICgoCAICAgCAzVW7i+PTzyZGm/8/DbPZ2kf9W3bfcdWVnafzxve2n/f+UGgoCAICAgCAgIAgKCgIAgICAICAgCAoKAgCAgIAgMWmvjHk8bTG+LB0t7PA4Mp7E2erC1tvDcn7uBICAgCAgIAgKCgIAgICAICAgCAoKAgCAgIAgICAICgoCAICAgCAgIAgKCgIAgICAICAgCAoKAgCAgIAgICAKlz4UbDAanuq470d9xYGjNaa3NOPeHpYCAv/MnHAQEBAEBQUBAEBAQBAQEAQFBQEAQEBAE/gA+PaaX3z3NHAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 216x216 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2998)\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Check image dataset \"\"\"\n",
    "    \n",
    "# get single random image batch from test dataset\n",
    "for d in random.sample(list(test_loader), 1):\n",
    "\n",
    "    # unpack image data, label from sample\n",
    "    imgs, _ = d\n",
    "\n",
    "    # push images to gpu\n",
    "    imgs = imgs.to(device)\n",
    "\n",
    "    print(imgs[0].shape)\n",
    "\n",
    "    # get single image from gpu\n",
    "    img = np.transpose(imgs[0].cpu().numpy(), [1,2,0])\n",
    "\n",
    "    plt.figure(figsize = (3,3))\n",
    "\n",
    "    # plot input image\n",
    "    plt.subplot(111)\n",
    "    plt.imshow(np.squeeze(img))\n",
    "    plt.xticks([]); plt.yticks([])\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(_[0])\n",
    "\n",
    "    break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Initialize the network \"\"\"\n",
    "\n",
    "# init vae model\n",
    "#net = VAE().to(device)\n",
    "\n",
    "# init model with specified hyperparams\n",
    "#net = VAE(fDim=16, kern=(1), zDim=32, imgShape=(25,25)).to(device)\n",
    "net = VAE().to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Initialize the Adam optimizer \"\"\"\n",
    "\n",
    "# init adam optimiser\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.2065, -0.0701,  0.1090, -0.2831,  0.0065,  0.2584,  0.1136, -0.2713])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.encConv1.bias.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x216 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(6474)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "The following part takes a random image from test loader to feed into the VAE.\n",
    "Both the original image and generated image from the distribution are shown.\n",
    "\"\"\"\n",
    "\n",
    "# set model eval state\n",
    "net.eval()\n",
    "\n",
    "# without compute gradients\n",
    "with torch.no_grad():\n",
    "    \n",
    "    # get single random image batch from test dataset\n",
    "    for d in random.sample(list(test_loader), 1):\n",
    "        \n",
    "        # unpack image data, label from sample\n",
    "        imgs, _ = d\n",
    "        \n",
    "        # push images to gpu\n",
    "        imgs = imgs.to(device)\n",
    "        \n",
    "        # get single image from gpu\n",
    "        img = np.transpose(imgs[0].cpu().numpy(), [1,2,0])\n",
    "        \n",
    "        plt.figure(figsize = (6,3))\n",
    "        \n",
    "        # plot input image\n",
    "        plt.subplot(121)\n",
    "        plt.imshow(np.squeeze(img))\n",
    "        plt.xticks([]); plt.yticks([])\n",
    "        \n",
    "        # compute output image\n",
    "        out, mu, logVAR = net(imgs)\n",
    "        \n",
    "        # get output image from gpu, reshape\n",
    "        outimg = np.transpose(out[0].cpu().numpy(), [1,2,0])\n",
    "        \n",
    "        # plot output image\n",
    "        plt.subplot(122)\n",
    "        plt.imshow(np.squeeze(outimg))\n",
    "        plt.xticks([]); plt.yticks([])\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(_[0])\n",
    "        \n",
    "        break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Loss 1043.2412109375\n",
      "Epoch 1: Loss 1024.7623291015625\n",
      "Epoch 2: Loss 1047.15673828125\n",
      "Epoch 3: Loss 1048.8431396484375\n",
      "Epoch 4: Loss 1015.9406127929688\n",
      "Epoch 5: Loss 1032.203369140625\n",
      "Epoch 6: Loss 1019.0953369140625\n",
      "Epoch 7: Loss 1018.7611694335938\n",
      "Epoch 8: Loss 1005.8056640625\n",
      "Epoch 9: Loss 1064.1639404296875\n",
      "Epoch 10: Loss 1049.5447998046875\n",
      "Epoch 11: Loss 1013.2725830078125\n",
      "Epoch 12: Loss 1027.72265625\n",
      "Epoch 13: Loss 1028.1796875\n",
      "Epoch 14: Loss 1036.0330810546875\n",
      "Epoch 15: Loss 1006.1487426757812\n",
      "Epoch 16: Loss 1020.04541015625\n",
      "Epoch 17: Loss 1027.6861572265625\n",
      "Epoch 18: Loss 1032.3779296875\n",
      "Epoch 19: Loss 1054.9027099609375\n",
      "Epoch 20: Loss 1035.2137451171875\n",
      "Epoch 21: Loss 1031.95263671875\n",
      "Epoch 22: Loss 1029.55078125\n",
      "Epoch 23: Loss 1060.551513671875\n",
      "Epoch 24: Loss 1063.203125\n",
      "Epoch 25: Loss 1084.8900146484375\n",
      "Epoch 26: Loss 1023.0577392578125\n",
      "Epoch 27: Loss 1005.277099609375\n",
      "Epoch 28: Loss 1011.7184448242188\n",
      "Epoch 29: Loss 1038.37353515625\n",
      "Epoch 30: Loss 1024.9915771484375\n",
      "Epoch 31: Loss 1086.8267822265625\n",
      "Epoch 32: Loss 1051.5479736328125\n",
      "Epoch 33: Loss 1035.855712890625\n",
      "Epoch 34: Loss 1041.5494384765625\n",
      "Epoch 35: Loss 1032.7965087890625\n",
      "Epoch 36: Loss 1044.0318603515625\n",
      "Epoch 37: Loss 1053.093017578125\n",
      "Epoch 38: Loss 1045.9454345703125\n",
      "Epoch 39: Loss 1036.3121337890625\n",
      "Epoch 40: Loss 1043.96826171875\n",
      "Epoch 41: Loss 1002.1973876953125\n",
      "Epoch 42: Loss 1032.290283203125\n",
      "Epoch 43: Loss 1074.170166015625\n",
      "Epoch 44: Loss 1057.9288330078125\n",
      "Epoch 45: Loss 1053.198486328125\n",
      "Epoch 46: Loss 1055.70166015625\n",
      "Epoch 47: Loss 1062.88671875\n",
      "Epoch 48: Loss 1014.2180786132812\n",
      "Epoch 49: Loss 1031.6343994140625\n",
      "Epoch 50: Loss 1053.685302734375\n",
      "Epoch 51: Loss 1046.667724609375\n",
      "Epoch 52: Loss 1065.7237548828125\n",
      "Epoch 53: Loss 1030.541259765625\n",
      "Epoch 54: Loss 1053.5843505859375\n",
      "Epoch 55: Loss 1014.1849975585938\n",
      "Epoch 56: Loss 1055.5625\n",
      "Epoch 57: Loss 1063.1455078125\n",
      "Epoch 58: Loss 1044.2628173828125\n",
      "Epoch 59: Loss 1076.42822265625\n",
      "Epoch 60: Loss 1084.1395263671875\n",
      "Epoch 61: Loss 1075.2508544921875\n",
      "Epoch 62: Loss 1059.9346923828125\n",
      "Epoch 63: Loss 1059.4100341796875\n",
      "Epoch 64: Loss 1061.396484375\n",
      "Epoch 65: Loss 1019.022705078125\n",
      "Epoch 66: Loss 990.5081176757812\n",
      "Epoch 67: Loss 1013.2836303710938\n",
      "Epoch 68: Loss 1037.001708984375\n",
      "Epoch 69: Loss 1062.691650390625\n",
      "Epoch 70: Loss 1038.0833740234375\n",
      "Epoch 71: Loss 1061.087646484375\n",
      "Epoch 72: Loss 1022.2802734375\n",
      "Epoch 73: Loss 1011.8330688476562\n",
      "Epoch 74: Loss 1060.3206787109375\n",
      "Epoch 75: Loss 1035.220703125\n",
      "Epoch 76: Loss 1010.5382080078125\n",
      "Epoch 77: Loss 1043.3427734375\n",
      "Epoch 78: Loss 997.512451171875\n",
      "Epoch 79: Loss 1030.404052734375\n",
      "Epoch 80: Loss 995.5020141601562\n",
      "Epoch 81: Loss 1042.193115234375\n",
      "Epoch 82: Loss 1002.7776489257812\n",
      "Epoch 83: Loss 1028.4344482421875\n",
      "Epoch 84: Loss 1067.6458740234375\n",
      "Epoch 85: Loss 986.8396606445312\n",
      "Epoch 86: Loss 1030.9140625\n",
      "Epoch 87: Loss 1033.9322509765625\n",
      "Epoch 88: Loss 1032.849609375\n",
      "Epoch 89: Loss 1038.2545166015625\n",
      "Epoch 90: Loss 1057.263427734375\n",
      "Epoch 91: Loss 1050.3419189453125\n",
      "Epoch 92: Loss 1034.244140625\n",
      "Epoch 93: Loss 1006.260009765625\n",
      "Epoch 94: Loss 1045.7728271484375\n",
      "Epoch 95: Loss 1024.892822265625\n",
      "Epoch 96: Loss 1040.924072265625\n",
      "Epoch 97: Loss 1035.6085205078125\n",
      "Epoch 98: Loss 1037.6353759765625\n",
      "Epoch 99: Loss 1113.4688720703125\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Training the network for a given number of epochs\n",
    "The loss after every epoch is printed\n",
    "\"\"\"\n",
    "\n",
    "# set train state\n",
    "net.train()\n",
    "\n",
    "num_epochs = 100\n",
    "\n",
    "# iterate over epochs\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    # iterate over training dataset by batch\n",
    "    for idx, d in enumerate(train_loader, 0):\n",
    "        \n",
    "        # unpack batch images and labels\n",
    "        imgs, _ = d\n",
    "        \n",
    "        # push images to gpu\n",
    "        imgs = imgs.to(device)\n",
    "        \n",
    "        #print('batch ',imgs.shape)\n",
    "\n",
    "        # Feeding a batch of images into the network to obtain the output image, mu, and logVar\n",
    "        out, mu, logVar = net(imgs)\n",
    "        \n",
    "        #print('out ', out.shape)\n",
    "\n",
    "        # The loss is the BCE loss combined with the KL divergence to ensure the distribution is learnt\n",
    "        kl_divergence = 0.5 * torch.sum(-1 - logVar + mu.pow(2) + logVar.exp())\n",
    "        loss = F.binary_cross_entropy(out, _)\n",
    "        #loss = (optimised power value from labels - observed power)^2 for supervised model\n",
    "\n",
    "        # Backpropagation based on the loss\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print('Epoch {}: Loss {}'.format(epoch, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x216 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2818)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "The following part takes a random image from test loader to feed into the VAE.\n",
    "Both the original image and generated image from the distribution are shown.\n",
    "\"\"\"\n",
    "\n",
    "# set model eval state\n",
    "net.eval()\n",
    "\n",
    "# without compute gradients\n",
    "with torch.no_grad():\n",
    "    \n",
    "    # get single random image batch from test dataset\n",
    "    for d in random.sample(list(test_loader), 1):\n",
    "        \n",
    "        # unpack image data, label from sample\n",
    "        imgs, _ = d\n",
    "        \n",
    "        # push images to gpu\n",
    "        imgs = imgs.to(device)\n",
    "        \n",
    "        # get single image from gpu\n",
    "        img = np.transpose(imgs[0].cpu().numpy(), [1,2,0])\n",
    "        \n",
    "        plt.figure(figsize = (6,3))\n",
    "        \n",
    "        # plot input image\n",
    "        plt.subplot(121)\n",
    "        plt.imshow(np.squeeze(img))\n",
    "        plt.xticks([]); plt.yticks([])\n",
    "        \n",
    "        # compute output image\n",
    "        out, mu, logVAR = net(imgs)\n",
    "        \n",
    "        # get output image from gpu, reshape\n",
    "        outimg = np.transpose(out[0].cpu().numpy(), [1,2,0])\n",
    "        \n",
    "        # plot output image\n",
    "        plt.subplot(122)\n",
    "        plt.imshow(np.squeeze(outimg))\n",
    "        plt.xticks([]); plt.yticks([])\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(_[0])\n",
    "        \n",
    "        break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/model/cnn-vae-f8-k3-z256-i115-v01.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [94]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# set model checkpoint path inc. hyperparams\u001b[39;00m\n\u001b[1;32m      4\u001b[0m _path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../data/model/cnn-vae-f\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m-k\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m-z\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m-i\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m-v\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.pth\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;241m8\u001b[39m,\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m256\u001b[39m,\u001b[38;5;241m115\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m01\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m#'epoch': epoch,\u001b[39;49;00m\n\u001b[1;32m      8\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m#'model_state_dict': model.state_dict(),\u001b[39;49;00m\n\u001b[1;32m      9\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel_state_dict\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mnet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43moptimizer_state_dict\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m#'loss': loss,\u001b[39;49;00m\n\u001b[1;32m     12\u001b[0m \u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/serialization.py:377\u001b[0m, in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[1;32m    341\u001b[0m \u001b[38;5;124;03m\"\"\"save(obj, f, pickle_module=pickle, pickle_protocol=DEFAULT_PROTOCOL, _use_new_zipfile_serialization=True)\u001b[39;00m\n\u001b[1;32m    342\u001b[0m \n\u001b[1;32m    343\u001b[0m \u001b[38;5;124;03mSaves an object to a disk file.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;124;03m    >>> torch.save(x, buffer)\u001b[39;00m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    375\u001b[0m _check_dill_version(pickle_module)\n\u001b[0;32m--> 377\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mwb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[1;32m    378\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[1;32m    379\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m _open_zipfile_writer(opened_file) \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/serialization.py:231\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    230\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 231\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    232\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    233\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/serialization.py:212\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[0;32m--> 212\u001b[0m     \u001b[38;5;28msuper\u001b[39m(_open_file, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/model/cnn-vae-f8-k3-z256-i115-v01.pth'"
     ]
    }
   ],
   "source": [
    "''' save model checkpoint '''\n",
    "\n",
    "# set model checkpoint path inc. hyperparams\n",
    "_path = '../data/model/cnn-vae-f{}-k{}-z{}-i{}-v{}.pth'.format(8,3,256,115,'01')\n",
    "\n",
    "torch.save({\n",
    "            #'epoch': epoch,\n",
    "            #'model_state_dict': model.state_dict(),\n",
    "            'model_state_dict': net.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            #'loss': loss,\n",
    "}, _path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' load model checkpoint from file '''\n",
    "\n",
    "# init model and optimiser\n",
    "#model = TheModelClass(*args, **kwargs)\n",
    "#optimizer = TheOptimizerClass(*args, **kwargs)\n",
    "\n",
    "#_path = '../data/model/cnn-vae-f16-k5-z128-i121-v01.pth'\n",
    "#_path = '../data/model/cnn-vae-f16-k3-z128-i115-v01.pth'\n",
    "_path = '../data/model/cnn-vae-f8-k3-z256-i115-v01.pth'\n",
    "\n",
    "# load checkpoint\n",
    "checkpoint = torch.load(_path)\n",
    "\n",
    "# update model\n",
    "#model.load_state_dict(checkpoint['model_state_dict'])\n",
    "net.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "# update optimiser\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "# set epoch and loss\n",
    "#epoch = checkpoint['epoch']\n",
    "#loss = checkpoint['loss']\n",
    "\n",
    "# set model train/eval state\n",
    "#model.eval()\n",
    "#model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
