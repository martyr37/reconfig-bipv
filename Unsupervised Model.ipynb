{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1495b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "from solar_module import SolarModule, generate_shading, generate_gaussian\n",
    "from solar_cell import all_series_bypass, SP_interconnection, TCT_interconnection\n",
    "from string_to_embedding import string_to_embedding, super_to_embedding\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc84d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, imgchannels=1, fdim=8, zdim=32, mdim=4096, kern=3, imgshape=(3,3), stride=1, pad=1):\n",
    "        \n",
    "        super(Model, self).__init__()\n",
    "        \n",
    "        # First 2D convolutional layer, taking in 1 input channel (image),\n",
    "        # outputting 8 convolutional features, with a square kernel size of 2\n",
    "        self.conv1 = nn.Conv2d(imgchannels, fdim, kern, stride, pad) # (1, 8, 3, 1)\n",
    "\n",
    "        # Second 2D convolutional layer, taking in the 8 input layers,\n",
    "        # outputting 32 convolutional features, with a square kernel size of 2\n",
    "        self.conv2 = nn.Conv2d(fdim, zdim, kern, stride, pad) # (8, 32, 3, 1, 1)\n",
    "\n",
    "        # Designed to ensure that adjacent pixels are either all 0s or all active\n",
    "        # with an input probability\n",
    "        #self.dropout1 = nn.Dropout2d(0.25)\n",
    "        #self.dropout2 = nn.Dropout2d(0.5)\n",
    "        \n",
    "        ldim = zdim*imgshape[0]*imgshape[1]\n",
    "        self.ldim = ldim\n",
    "\n",
    "        # First fully connected layer\n",
    "        self.fc1 = nn.Linear(ldim, mdim) #add a middle linear layer, with size \n",
    "        self.fc3 = nn.Linear(mdim, mdim)\n",
    "        outdim = (imgshape[0]*imgshape[1])**2*3+imgshape[0]*imgshape[1]*2\n",
    "        #outdim = (imgshape[0]*imgshape[1])**2*3\n",
    "        self.fc2 = nn.Linear(mdim, outdim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        #print(x.shape)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        #print(x.shape)\n",
    "        \n",
    "        #x = self.dropout1(x)\n",
    "        x = x.view(-1, self.ldim)\n",
    "        #print(x.shape)\n",
    "        \n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = F.relu(x)\n",
    "        #print(x.shape)\n",
    "        #x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        #print(x.shape)\n",
    "        \n",
    "        x = F.relu(x)\n",
    "        #x = x.bool()\n",
    "        \n",
    "        return x\n",
    "\n",
    "model = Model(imgshape=(10, 6)).to(device)\n",
    "print(model)\n",
    "print(device)\n",
    "for param in model.parameters():\n",
    "  print(param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8a89e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_smap = generate_gaussian(10, 10, 6)\n",
    "plt.imshow(rand_smap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5d12bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_smap = torch.Tensor(np.expand_dims(rand_smap,0))\n",
    "print(rand_smap)\n",
    "print(rand_smap.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9012d420",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model(rand_smap)\n",
    "print(result)\n",
    "print(result.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4945c1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Initialize Hyperparameters \"\"\"\n",
    "#10,000 iterations, batching of 32\n",
    "\n",
    "#1e-3 learning rate\n",
    "\n",
    "# vary each training parameter individually, run multiple experiments. Checkpoint each model, filename w/ parameters\n",
    "\n",
    "batch_size = 32 # 32-64 is advisable\n",
    "learning_rate = 1e-3 # sensitivity of gradient descent\n",
    "num_epochs = 1 # iterations over entire dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3876f072",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Generate 10x6 Shading Maps\"\"\"\n",
    "filename = '10x6shading_maps.csv'\n",
    "maps = []\n",
    "for x in range(0, 1000):\n",
    "    rand_smap = generate_gaussian(10, 10, 6)\n",
    "    maps.append(rand_smap)\n",
    "\n",
    "maps = pd.Series(maps, name='Shading Maps')\n",
    "maps.to_csv(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d0f1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Load Shading Maps\"\"\"\n",
    "read_in = pd.read_csv('10x6shading_maps.csv')\n",
    "    \n",
    "def convert_to_array(string):\n",
    "    a = np.matrix(string).reshape(10, 6)\n",
    "    a = np.array(a)\n",
    "    return a\n",
    "shading_series = [convert_to_array(s) for s in read_in['Shading Maps']]\n",
    "data = [torch.Tensor(np.expand_dims(s,0)) for s in shading_series]\n",
    "train_loader = torch.utils.data.DataLoader(data, batch_size=batch_size, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27435dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Create custom dataset \"\"\"\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, df, rotate=None):\n",
    "        self.df = df\n",
    "        self.rotate = rotate # for rotation of shading maps?\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        #shading_map = self.df.iloc[index, 0]\n",
    "        shading_map = shading_series[index]\n",
    "        #print(shading_map)\n",
    "        shading_map = torch.Tensor(shading_map)/10\n",
    "        shading_map = shading_map.unsqueeze(0)\n",
    "        \n",
    "        return shading_map\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(df)\n",
    "\n",
    "dataset = CustomDataset(train_loader)\n",
    "print(dataset.__getitem__(2))\n",
    "print(dataset.__getitem__(2)[0].shape)\n",
    "print(len(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b9d7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(imgshape=(10, 6)).to(device)\n",
    "print(model)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af16a24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69429178",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio_list = []\n",
    "self_connections = []\n",
    "conflicting_connections = []\n",
    "loss_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca8df41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate over epochs\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    # iterate over training dataset by batch\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "\n",
    "            shading_map = data\n",
    "            shading_map = shading_map.to(device)\n",
    "            result = model(shading_map)\n",
    "            \n",
    "            average_ratio = 0\n",
    "            self_c = 0\n",
    "            conflicting_c = 0\n",
    "            for j in range(0, batch_size):\n",
    "                embedding = result[j]\n",
    "                embedding = np.squeeze(embedding.detach())\n",
    "                embedding, terminal_array = embedding[:10800], embedding[10800:]\n",
    "                embedding = embedding.reshape(10, 6, 10, 6, 3)\n",
    "                terminal_array = terminal_array.reshape(10, 6, 2)\n",
    "                \n",
    "                series_connections = torch.count_nonzero(embedding[...,0]) + torch.count_nonzero(embedding[...,1])\n",
    "                parallel_connections = torch.count_nonzero(embedding[...,2])\n",
    "                if parallel_connections == 0:\n",
    "                    average_ratio += 0\n",
    "                elif parallel_connections > 0:\n",
    "                    average_ratio += (series_connections / parallel_connections)\n",
    "                \n",
    "                moduleobj = SolarModule(10, 6)\n",
    "                moduleobj.embedding = np.array(embedding.numpy(), dtype=bool)\n",
    "                moduleobj.terminal_array = np.array(terminal_array.numpy(), dtype=bool)\n",
    "                filtering = moduleobj.filter_embedding()\n",
    "                self_c += filtering[0]\n",
    "                conflicting_c += filtering[1]\n",
    "                \n",
    "            average_ratio /= batch_size\n",
    "            self_c /= batch_size\n",
    "            conflicting_c /= batch_size\n",
    "            ratio_list.append(float(average_ratio))\n",
    "            self_connections.append(self_c)\n",
    "            conflicting_connections.append(conflicting_c)\n",
    "            \n",
    "            #loss = torch.mean(entropy_loss(result, embeddings),axis=1)\n",
    "            #loss = torch.mean(loss*(1-power))\n",
    "            \n",
    "            loss = torch.tensor(conflicting_c, requires_grad=True)\n",
    "            \n",
    "            loss_list.append(float(loss))\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            print(i, ratio_list[i], self_connections[i], conflicting_connections[i], loss)\n",
    "    \n",
    "    print('Epoch {}: Loss {}'.format(epoch, loss))\n",
    "    \n",
    "# create validation/evaluation batch of 10 or so shading maps, calculate the power from that\n",
    "# visualise embeddings or circuit diagram against shading map\n",
    "\n",
    "# use embeddings generated by superstrings for supervised approach. \n",
    "# pre-train on training_data.csv\n",
    "# binary cross entropy between model and training data\n",
    "\n",
    "# sort training_data.csv by performance (power)\n",
    "# train same model by binary cross entropy\n",
    "# use entire dataset, then use higher performing subset, then a subset of that, etc. \n",
    "# label = embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8dc95e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in loss_graph:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a2636a",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' save model checkpoint '''\n",
    "\n",
    "# set model checkpoint path inc. hyperparams\n",
    "_path = 'unsupervised-f{}-k{}-z{}-i{}-v{}.pth'.format(8,3,256,115,'01')\n",
    "torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': loss,\n",
    "}, _path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49cb4b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" load model checkpoint \"\"\"\n",
    "''' load model checkpoint from file '''\n",
    "\n",
    "# init model and optimiser\n",
    "#model = TheModelClass(*args, **kwargs)\n",
    "#optimizer = TheOptimizerClass(*args, **kwargs)\n",
    "\n",
    "#_path = '../data/model/cnn-vae-f16-k5-z128-i121-v01.pth'\n",
    "#_path = '../data/model/cnn-vae-f16-k3-z128-i115-v01.pth'\n",
    "_path = 'Checkpoints/Batch42FromInit.pth'\n",
    "\n",
    "# load checkpoint\n",
    "checkpoint = torch.load(_path)\n",
    "\n",
    "# update model\n",
    "#model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "# update optimiser\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "# set epoch and loss\n",
    "epoch = checkpoint['epoch']\n",
    "loss = checkpoint['loss']\n",
    "\n",
    "# set model train/eval state\n",
    "#model.eval()\n",
    "#model.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
